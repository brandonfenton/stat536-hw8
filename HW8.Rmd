---
title: "HW 8"
author: "Brandon Fenton and Matt Pettigrew"
date: "November 26, 2016"
output:
  pdf_document:
    fig_height: 3
    fig_width: 5
  html_document:
    fig_width: 5
  word_document:
    fig_height: 3
    fig_width: 5

header-includes: \usepackage{float} \usepackage{bm} \usepackage{amsmath} \usepackage{amssymb} \usepackage{microtype} \usepackage{wasysym}
---

```{r setup, include=FALSE, echo=F}
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

library(grid)
library(gridExtra)
library(pander)
library(dplyr)
library(ggplot2)
library(effects)
library(ggfortify)
library(parallel)

panderOptions('missing', "-")

pander_lm <-function (fit, ...)
{
  fit.sum <- summary(fit)
  fit.coef <- fit.sum$coefficients
  fit.ttable <- matrix(nrow=length(fit.sum$aliased), ncol=4)
  colnames(fit.ttable) <- colnames(fit.sum$coefficients)
  rownames(fit.ttable) <- names(fit.sum$aliased)

  notna <- as.vector(which(!fit.sum$aliased))
  fit.ttable[notna,] <- fit.coef
  fit.ttable <- as.data.frame(fit.ttable)
  fit.ttable$`Pr(>|t|)` <- ifelse(fit.ttable$`Pr(>|t|)` < 0.0001, "<0.0001",
                                     sprintf("%.4f", fit.ttable$`Pr(>|t|)`))
  

  pander(fit.ttable, ...)
}

pander_anova <-function (fit, ...)
{
  fit.anova <- anova(fit)
  fit.anova$`Pr(>F)` <- ifelse(fit.anova$`Pr(>F)` < 0.0001, "<0.0001",
                                  sprintf("%.4f", fit.anova$`Pr(>F)`))

pander(fit.anova, ...)
}

clust <- makeCluster(detectCores())

```

1) _What do they provide for a reason for removing the quadratic trend? Why do they want to use a trend model with limited flexibility?_

    The authors justify removing the quadratic trend on the basis that ARMA models rely on the assumption of stationarity.  The goal in removing the trend is to obtain residuals with a constant mean of zero and constant variance.  If the residuals satisfy these properties then it stationarity is assumed, allowing an ARMA model to be utilized.  Since the goal was to remove only the trend and not the variation around the trend, "simple, parsimonious methods" were given preference.  This rules out many possible detrending methods; in particular methods with too much flexibility (for example, non-stiff splines) are unsuitable since they can lead to overfitting.
    
2) _How do they determine the order of the trend model from none to quadratic? Are there any potential criticisms of the way they do this? Do not discuss the potential issue with doing these tests without accounting for potential autocorrelation – we know that the SEs would change if these tests were conducted using models that account for autocorrelation._

    The order of the trend model was determined using "standard statistical procedures(e.g., _t_-test," with "either a linear or quadratic trend" used for each model.  The first problem with this approach is that the wording is imprecise: were t-tests the only basis for comparison?  Does the word "either" imply that the quadratic models did not contain a linear term (which they should)?  What were the p-values?
    The second problem with their method is that it was applied without any thought given to consistency.  The only species-variable combination with the same trend fit for each observation is $\delta_p$ (the isotropic composition, $\delta^{13}C ( \permil )$, of plant tissue) for Douglas Fir.  Unless there is a domain-knowledge-specific explanation  -- if there is one, it isn't given -- this practice seems questionable at best.

3) _Figure 3 contains two ACF plots. Discuss the patterns in the two SACFs (left two panels of Figure 3)._ 

    Both SACF plots exhibit similar patterns.  Over the first few lags there is positive autocorrelation.  For mean Douglas-fir the ACF exceeds the 2SE band for the 2nd lag, while the mean western white pine exceeds the band at both the 1st and 2nd lag indicating that there is strong evidencde that the autocorrelation is not equal to zero.  Each trend flips to a negative autocorrelation at lag four and five respectively.  Then, both switch back to positive at lags of 11 and 12 respectively.  The pattern of both trends are that of a sine wave and are visibility damping as the number of lags increases.

4) _Note that their bounds for testing in Figure 3 are different from the formula provided in the book – we will discuss the modification I think they used later. What is the sample size (length of time series!) for the mean douglas-fir and mean western white pine time series. Based on these sample sizes, what cut-offs would you plot on Figure 3 based on the approximate result we discussed in class? How do theirs differ?_

    The sample size for Douglas fir is 85.  For the white the pine the sample size is 77.  Therefore the cut-offs based on the approximation we discussed are $\pm \frac{2}{\sqrt{8}} = \pm 0.7071$ for the douglas fir and $\pm \frac{2}{\sqrt{10}} = \pm 0.6325$ for the white pine.  Our approximation differs from the 2SE method used by the author, in that our approximation is constant across different lags.  The approximation used by the authors increases slightly as the size of the lags increases

5) _We discussed four different reasons for doing time series modeling at the beginning of the semester (pages 1 and 2 of lecture notes). Of those four reasons, which one does their use of ARMA models fall into and why? There may be multiple choices that are reasonable here, so the argument you make is important. I am not talking about what you could do with their models, just what they did do with them and their reason for employing them._

    Of the four reasons for doing time-series modeling -- description/understanding, accommodation, monitoring/control, and forecasting -- the primary purpose of this paper is description/understanding ("we looked for he presence and structure of autocorrelation"), albeit with the aim of facilitating accommodation in future studies.  Although the models were used to examine scientific questions of interest such as what the relationship between discrimination ($\Delta$) and height for each species might be, this was not the primary purpose of the paper.

6) _Use arima.sim to simulate from Delta response AR(2) models for either the mean D or mean W discussed in Table 4 generating the same number of observations as in their time series (see Table 2). You can use a standard deviation for the white noise process of 0.4. This simulation is just concerned with the residual process after removing the trend. Plot a simulated fake time series, the SACF from the fake time series, and compare the results to those that you can find in the paper in Figure 3. How are your results similar or different from their results? Simulate a second series and discuss the differences in your two simulations._

```{r p6}
require(TSA)
set.seed(153)
y1 <- arima.sim(model=list(ar=c(0.37,0.15)), n=77, sd=0.4)
y2 <- arima.sim(model=list(ar=c(0.37,0.15)), n=77, sd=0.4)

plot(1:77, y1, type="l", ylab=expression(delta[p]* ' Simulation 1'), main='White Pine')
plot(1:77, y2, type="l", ylab=expression(delta[p]* ' Simulation 2'), main='White Pine')

par(mfrow=c(1,2))
acf(y1, main='Series Simulation 1')
acf(y2, main='Series Simulation 2')
```

The first simulation has a large but not "significant" spike at the first lag and a single significant spike at lag 14.  This likely indicates that there is not autocorrelation present in the first simulated sample based on the random generation process. This constrasts the authors' results which had two significant spikes at the first two lags.  Furthermore, the damping sinusoidal pattern is not present in the simulated sample.  

However, the second simulation has autocorrelation present in the first two lags as expected based on how the data was generated.  Again the data does not exhibit a strong damping sinusoidal pattern, however one could make the case that sine pattern is present albeit a different one than in the paper.

7) _Use R to find the roots of your selected AR(2) model based on their published coefficients and note what that tells you about the properties of the AR(2) process under consideration._

```{r p7}
zvals <- seq(-4.2,1.7, by=0.01)
fz <- function(x) 1 - 0.37*(x) - 0.15*(x^2)
plot(x=zvals, y=fz(zvals), type="l", ylab = expression(paste("1-0.37z-0.15",z^2)),
     xlab="z")
abline(h=0, lty=2)
```

Using `polyroot(c(1,-0.37,-0.15))`, the resultant roots are `r polyroot(c(1,-0.37,-0.15))[1]` and `r polyroot(c(1,-0.37,-0.15))[2]`.  Both of these numbers have absolute values greater than one, which indicates that the AR(2) process is stationary.
